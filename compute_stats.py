# compute_stats.py

import re
from pathlib import Path

import pandas as pd

# â”€â”€â”€ Einstellungen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RES_DIR    = Path("results")
PATTERN    = "*_results.csv"
OUT_CONS   = RES_DIR / "constellation_stats.csv"
OUT_COMP   = RES_DIR / "complexity_stats.csv"
METRICS    = ["duration_ms", "avg_cpu", "avg_mem"]
COMPLEXITY_ORDER = ["easy", "medium", "complex", "very_complex", "create", "update", "delete"]

# â”€â”€â”€ Complexity-Mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def map_complexity(q: int) -> str:
    if   1  <= q <=  3:  return "easy"
    if   4  <= q <=  6:  return "medium"
    if   7  <= q <=  9:  return "complex"
    if  10 <= q <= 12:  return "very_complex"
    if  13 <= q <= 16:  return "create"
    if  17 <= q <= 20:  return "update"
    return "delete"

# â”€â”€â”€ CSV laden und Grunddaten aufbereiten â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_csv(path: Path) -> pd.DataFrame:
    users = int(re.match(r"(\d+)_", path.name).group(1))
    df = pd.read_csv(path)
    df = df[df["phase"] == "steady"]
    df["users"]      = users
    df["variant"]    = df["db"] + "_" + df["mode"]
    df["complexity"] = pd.Categorical(
        df["query_no"].astype(int).map(map_complexity),
        categories=COMPLEXITY_ORDER,
        ordered=True
    )
    return df

# â”€â”€â”€ 1. Alle CSVs einlesen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
frames = [load_csv(f) for f in RES_DIR.glob(PATTERN)]
df = pd.concat(frames, ignore_index=True)

# â”€â”€â”€ 2. Stats pro Konstellation (users, concurrency, variant) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cons_stats = (
    df
    .groupby(["users", "concurrency", "variant"], observed=True, as_index=False)[METRICS]
    .agg(["mean", "std", "var"])
)
# Spalten flachmachen: nur Metrik-MultiIndex-EintrÃ¤ge mit nicht-leerem Unterlabel
new_cols = []
for col in cons_stats.columns:
    if isinstance(col, tuple) and col[1]:
        new_cols.append(f"{col[0]}_{col[1]}")
    else:
        new_cols.append(col[0] if isinstance(col, tuple) else col)
cons_stats.columns = new_cols

cons_stats.to_csv(OUT_CONS, index=False)
print(f"ðŸ’¾ Konstellation-Stats â†’ {OUT_CONS}")

# â”€â”€â”€ 3. Stats pro Complexity (users, concurrency, variant, complexity) â”€â”€â”€â”€â”€
comp_stats = (
    df
    .groupby(["users", "concurrency", "variant", "complexity"], 
             observed=True, as_index=False)[METRICS]
    .agg(["mean", "std", "var"])
)
# Spalten flachmachen wie oben
new_cols = []
for col in comp_stats.columns:
    if isinstance(col, tuple) and col[1]:
        new_cols.append(f"{col[0]}_{col[1]}")
    else:
        new_cols.append(col[0] if isinstance(col, tuple) else col)
comp_stats.columns = new_cols

# Sortieren in definierter Reihenfolge
comp_stats["complexity"] = pd.Categorical(
    comp_stats["complexity"],
    categories=COMPLEXITY_ORDER,
    ordered=True
)
comp_stats = comp_stats.sort_values(
    ["users", "concurrency", "variant", "complexity"],
    ignore_index=True
)

comp_stats.to_csv(OUT_COMP, index=False)
print(f"ðŸ’¾ Complexity-Stats     â†’ {OUT_COMP}")
