import argparse          # Erm√∂glicht das Einlesen und Verarbeiten von Kommandozeilenargumenten
import json              # Zum Parsen von JSON-Dateien
import csv               # F√ºr das Schreiben von CSV-Dateien (z.‚ÄØB. f√ºr den Neo4j-Import)
import subprocess        # Zur Ausf√ºhrung externer Shell-Kommandos (z.‚ÄØB. Docker)
import time              # Zur Steuerung von Pausen und Wartezeiten
from pathlib import Path # Plattformunabh√§ngiges Arbeiten mit Dateipfaden
import re                # Regul√§re Ausdr√ºcke, z.‚ÄØB. f√ºr Datenbereinigung
import shutil            # Dateisystemoperationen (z.‚ÄØB. Ordner l√∂schen)
import os, math, sys
from neo4j import GraphDatabase                    # Offizieller Python-Treiber f√ºr Neo4j (Bolt-Verbindung)
from neo4j.exceptions import ServiceUnavailable    # Fehlerbehandlung bei nicht erreichbarem Neo4j-Service

# === Konfiguration der Arbeitsumgebung ===

CSV_DIR = Path(__file__).resolve().parent / "import"  # Verzeichnis f√ºr importierte CSV-Dateien

CONTAINER_NAME = "neo5_test_optimized"                   # Eindeutiger Name f√ºr den Docker-Container (Optimized-Version)
IMAGE_NAME = "neo5-optimized"                            # Name des zu verwendenden Docker-Images (Optimized-Version)
BASE_DIR = Path(__file__).resolve().parent            # Ordner, in dem DAS Skript liegt
RESULTS_DIR = (BASE_DIR / ".." / "results").resolve()


# === Tabellenstruktur f√ºr Optimized-Version ========================================
# Diese Struktur legt die CSV-Spaltennamen und -typen f√ºr alle Knoten fest,
# die im optimierten Graphmodell verwendet werden. Sie dient als Grundlage f√ºr
# den CSV-Export und anschlie√üenden Import in Neo4j.

NODE_TABLES = {
    "users": [
        "user_id:ID(User)",          # ‚á¢ Neo4j-Import-ID f√ºr User-Knoten
        "id:int",                    # ‚á¢ Fachliche ID (zur Beziehungserzeugung)
        "name", "email",             # ‚á¢ Basisdaten des Nutzers
        "created_at:datetime"        # ‚á¢ Registrierungsdatum
    ],

    "products": [
        "product_id:ID(Product)",
        "id:int",
        "name", "description",       # ‚á¢ Produktbezeichnung und -beschreibung
        "price:float", "stock:int",  # ‚á¢ Preisangabe und Lagerbestand
        "created_at:datetime", "updated_at:datetime"
    ],

    "categories": [
        "category_id:ID(Category)",
        "id:int",
        "name"                       # ‚á¢ Bezeichnung der Produktkategorie
    ],

    "addresses": [
        "address_id:ID(Address)",
        "id:int",
        "user_id:int",               # ‚á¢ Fremdbezug zum User
        "street", "city", "zip", "country",
        "is_primary:boolean"         # ‚á¢ Angabe, ob Hauptadresse
    ],

    "orders": [
        "order_id:ID(Order)",
        "id:int",
        "user_id:int", "status",     # ‚á¢ Nutzerreferenz und Auftragsstatus
        "total:float",
        "created_at:datetime", "updated_at:datetime"
    ],

    "payments": [
        "payment_id:ID(Payment)",
        "id:int",
        "order_id:int", "payment_method",
        "payment_status", "paid_at:datetime"
    ],

    "shipments": [
        "shipment_id:ID(Shipment)",
        "id:int",
        "order_id:int", "tracking_number",
        "shipped_at:datetime", "delivered_at:datetime", "carrier"
    ]
}


# === Mapping Node-Table ‚Üí Node-Typ ===============================================
# Diese Zuordnung definiert, welches Label (Node-Typ) in Neo4j f√ºr die jeweilige
# JSON- oder CSV-Tabelle verwendet wird. Sie wird beim Import ben√∂tigt, um die
# Knoten mit semantisch korrekten Typbezeichnungen zu versehen.

NODE_TYPES = {
    "users":       "User",       # ‚á¢ Benutzerkonten
    "products":    "Product",    # ‚á¢ Produkte (z.‚ÄØB. Artikel im Shop)
    "categories":  "Category",   # ‚á¢ Produktkategorien
    "addresses":   "Address",    # ‚á¢ Nutzeradressen
    "orders":      "Order",      # ‚á¢ Kundenbestellungen
    "payments":    "Payment",    # ‚á¢ Zahlungsinformationen
    "shipments":   "Shipment"    # ‚á¢ Versandinformationen
}


# === Relationship-Builder (Optimized-Modell) ================================
# Diese Struktur definiert, wie Beziehungen aus den JSON-Zeilen generiert werden.
# Sie enth√§lt sowohl klassische Referenzen (direkt aus Nodes) als auch umgewandelte
# fr√ºhere Join-Knoten, die nun als Beziehungen mit Properties modelliert sind.

RELATION_BUILDERS = {

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 1) Beziehungen direkt aus verkn√ºpften Knoten ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Diese Beziehungen entstehen aus Feldern innerhalb der urspr√ºnglichen Knoten,
    # z.‚ÄØB. user_id in Adressen oder Bestellungen.
    "user_address": lambda row: {
        "user_id:START_ID(User)":     row["user_id"],
        "address_id:END_ID(Address)": row["id"],
        ":TYPE":                      "HAS_ADDRESS"
    },
    "user_order": lambda row: {
        "user_id:START_ID(User)": row["user_id"],
        "order_id:END_ID(Order)": row["id"],
        ":TYPE":                  "PLACED"
    },
    "order_payment": lambda row: {
        "order_id:START_ID(Order)":   row["order_id"],
        "payment_id:END_ID(Payment)": row["id"],
        ":TYPE":                      "PAID_WITH"
    },
    "order_shipment": lambda row: {
        "order_id:START_ID(Order)":     row["order_id"],
        "shipment_id:END_ID(Shipment)": row["id"],
        ":TYPE":                        "SHIPPED"
    },

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2) Wunschliste ohne eigene Entit√§t ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Die Wishlist hat keinen eigenen Node, daher werden die Informationen direkt
    # als Beziehung mit Timestamp gespeichert.
    "user_wishlist": lambda row: {
        "user_id:START_ID(User)":     row["user_id"],
        "product_id:END_ID(Product)": row["product_id"],
        "created_at:datetime":        row.get("created_at"),
        ":TYPE":                      "WISHLISTED"
    },

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 3) Ehemalige Join-Knoten als Relationship mit Properties ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # In der optimierten Modellierung werden Join-Tabellen (z.‚ÄØB. cart_items, reviews)
    # nicht mehr als Knoten, sondern als Beziehungen mit Metainformationen umgesetzt.
    "order_contains": lambda row: {
        "id:int":                    row["id"],                # fr√ºhere ID aus OrderItem
        "order_id:START_ID(Order)":  row["order_id"],
        "product_id:END_ID(Product)":row["product_id"],
        "quantity:int":              row["quantity"],
        "price:float":               row["price"],
        ":TYPE":                     "CONTAINS"
    },
    "user_reviewed": lambda row: {
        "id:int":                    row["id"],                # fr√ºhere ID aus Review
        "user_id:START_ID(User)":    row["user_id"],
        "product_id:END_ID(Product)":row["product_id"],
        "rating:int":                row["rating"],
        "comment":                   row["comment"],
        "created_at:datetime":       row["created_at"],
        ":TYPE":                     "REVIEWED"
    },
    "user_cart": lambda row: {
        "id:int":                    row["id"],                # fr√ºhere ID aus CartItem
        "user_id:START_ID(User)":    row["user_id"],
        "product_id:END_ID(Product)":row["product_id"],
        "quantity:int":              row["quantity"],
        "added_at:datetime":         row["added_at"],
        ":TYPE":                     "HAS_IN_CART"
    },
    "user_viewed": lambda row: {
        "id:int":                    row["id"],                # fr√ºhere ID aus ProductView
        "user_id:START_ID(User)":    row["user_id"],
        "product_id:END_ID(Product)":row["product_id"],
        "viewed_at:datetime":        row["viewed_at"],
        ":TYPE":                     "VIEWED"
    },
    "user_purchased": lambda row: {
        "id:int":                    row["id"],                # fr√ºhere ID aus ProductPurchase
        "user_id:START_ID(User)":    row["user_id"],
        "product_id:END_ID(Product)":row["product_id"],
        "purchased_at:datetime":     row["purchased_at"],
        ":TYPE":                     "PURCHASED"
    },
}


# === Datenquelle je Beziehungstyp ==========================================
# Diese Zuordnung definiert, aus welcher Tabelle die jeweiligen Beziehungen gespeist werden.
# Wichtig, um beim CSV-Export gezielt die richtigen JSON-Dateien f√ºr die Relationship-Generierung zu laden.

RELATION_TABLE_SOURCES = {
    "user_address":   "addresses",         # Adresse enth√§lt user_id ‚Üí User ‚Üî Address
    "user_order":     "orders",            # Order enth√§lt user_id ‚Üí User ‚Üî Order
    "order_payment":  "payments",          # Payment enth√§lt order_id ‚Üí Order ‚Üî Payment
    "order_shipment": "shipments",         # Shipment enth√§lt order_id ‚Üí Order ‚Üî Shipment
    "user_wishlist":  "wishlists",         # JSON-basierte Wunschliste ohne Knoten
    "order_contains": "order_items",       # OrderItem als Beziehung zw. Order & Product
    "user_reviewed":  "reviews",           # Review als bewertende Beziehung
    "user_cart":      "cart_items",        # CartItem enth√§lt user_id & product_id
    "user_viewed":    "product_views",     # ProductView enth√§lt user_id & product_id
    "user_purchased": "product_purchases", # ProductPurchase enth√§lt user_id & product_id
}


def stop_neo4j_container():
    # Gibt eine Statusmeldung aus, dass versucht wird, den Container zu stoppen.
    print("üõë Stoppe laufenden Neo4j-Container falls aktiv ...")
    try:
        # F√ºhrt den Docker-Befehl zum Stoppen des Containers aus.
        # Der Befehl gibt keine Ausgabe zur√ºck, da stdout unterdr√ºckt wird.
        subprocess.run(["docker", "stop", CONTAINER_NAME], check=True, stdout=subprocess.DEVNULL)

        # Wiederholt bis zu 10 Mal (mit jeweils 1 Sekunde Pause), ob der Container vollst√§ndig beendet wurde.
        for _ in range(10):
            # Pr√ºft, ob ein Container mit dem angegebenen Namen noch vorhanden ist.
            result = subprocess.run(
                ["docker", "ps", "-a", "-q", "-f", f"name={CONTAINER_NAME}"],
                capture_output=True,
                text=True
            )
            # Wenn keine Container-ID zur√ºckgegeben wird, wurde der Container erfolgreich gestoppt.
            if not result.stdout.strip():
                print("‚úÖ Container wurde vollst√§ndig gestoppt.")
                return
            time.sleep(1)  # Wartezeit vor dem n√§chsten Versuch

    except Exception as e:
        # Gibt eine Fehlermeldung aus, falls beim Stoppen des Containers ein Problem auftritt.
        print(f"‚ö†Ô∏è  Fehler beim Stoppen: {e}")


def start_neo4j_container():
    # Gibt eine Statusmeldung aus, dass der Neo4j-Container gestartet wird.
    print("üöÄ Starte Neo4j-Container neu ...")

    # Ermittelt den absoluten Pfad zum lokalen Verzeichnis 'neo4j_data', 
    # in dem die persistente Datenhaltung erfolgen soll.
    data_volume_path = str((Path(__file__).resolve().parent / "neo4j_data").resolve())

    # Startet einen neuen Docker-Container mit den folgenden Parametern:
    # -d: im Hintergrund (detached mode)
    # --rm: Container wird nach dem Stoppen automatisch gel√∂scht
    # --name: setzt einen festen Containernamen
    # -e: √ºbergibt die Authentifizierungsdaten als Umgebungsvariable
    # -p: leitet Ports f√ºr HTTP (7474) und Bolt (7687) weiter
    # -v: bindet das Datenverzeichnis als Volume ein
    # IMAGE_NAME: definiertes Neo4j-Image
    subprocess.run([
        "docker", "run", "-d", "--rm",
        "--name", CONTAINER_NAME,
        "-e", "NEO4J_AUTH=neo4j/superpassword55",
        "-p", "7474:7474", "-p", "7687:7687",
        "-v", f"{data_volume_path}:/data",
        IMAGE_NAME
    ], check=True)

    # Wartet darauf, dass der Bolt-Endpunkt (Standardprotokoll von Neo4j) erreichbar ist.
    wait_for_bolt()

    # Gibt eine Erfolgsmeldung aus, sobald der Container aktiv ist.
    print("‚úÖ Container l√§uft.")


def fix_cypher_props(text):
    # Sucht nach Schl√ºsselbezeichnern in Cypher-Notation (z.‚ÄØB. name: ...) 
    # und wandelt diese in g√ºltige JSON-Schl√ºssel um (z.‚ÄØB. "name": ...).
    text = re.sub(r"(\w+):", r'"\1":', text)

    # Sucht nach nicht in Anf√ºhrungszeichen gesetzten String-Werten in Eigenschaftszuweisungen
    # (z.‚ÄØB. : admin) und erg√§nzt automatisch doppelte Anf√ºhrungszeichen (‚Üí : "admin").
    text = re.sub(r':\s*([A-Za-z_][A-Za-z0-9_]*)', r': "\1"', text)

    # Gibt den bereinigten Text zur√ºck, der nun syntaktisch korrekt ist f√ºr JSON oder Cypher-Mapping.
    return text


def convert_json_to_csv_refactored(json_file: Path, out_dir: Path):
    """Konvertiert ein gegebenes JSON-Datenobjekt in CSV-Dateien f√ºr Knoten und Relationen gem√§√ü der vorgegebenen Tabellenstruktur."""

    # L√§dt die JSON-Datei und stellt sicher, dass das Ausgabeverzeichnis existiert
    data = json.loads(Path(json_file).read_text(encoding="utf-8"))
    out_dir.mkdir(parents=True, exist_ok=True)

    # --------------------- Verarbeitung der Knoten-Tabellen ---------------------
    for table, header in NODE_TABLES.items():
        rows = data.get(table, [])
        if not rows:
            continue  # Tabelle ist im JSON nicht vorhanden oder leer

        # ‚ûä Erstellt eine Zuordnung zwischen Attributnamen und deren Typdefinitionen (z.‚ÄØB. 'boolean', 'int', '')
        type_by_key = {
            h.split(":")[0]: (h.split(":")[1] if ":" in h else "")
            for h in header
        }

        # Legt den Pfad zur Ausgabedatei fest und √∂ffnet sie im Schreibmodus
        csv_path = out_dir / f"{table}.csv"
        with csv_path.open("w", newline="", encoding="utf-8") as f_out:
            writer = csv.writer(f_out)
            writer.writerow(header)  # schreibt die Kopfzeile mit Typdefinitionen

            def resolve_value(row, key):
                # A) Direkter Zugriff: Wert ist im Datensatz vorhanden
                if key in row:
                    val = row[key]
                # B) Falls nur eine generische 'id' existiert, wird diese f√ºr Import-ID-Felder √ºbernommen
                elif key.endswith("_id") and "id" in row:
                    val = row["id"]
                # C) Andernfalls wird der Wert als fehlend markiert
                else:
                    val = None

                # ‚ûã Typabh√§ngige Umwandlung der Werte
                col_type = type_by_key.get(key, "")
                if col_type == "boolean":
                    # Fehlende Werte werden als 'false' interpretiert,
                    # vorhandene Booleans als lowercase-String zur√ºckgegeben
                    return "true" if bool(val) else "false"
                if val is None:
                    return ""  # fehlende Werte (nicht-boolean) werden als leere Zelle geschrieben
                return val  # alle anderen Typen: unbearbeitet zur√ºckgeben

            # Schreibt die aufbereiteten Datenzeilen in die CSV-Datei
            for row in rows:
                writer.writerow([resolve_value(row, k.split(":")[0])
                                 for k in header])

    # --------------------- Verarbeitung der Relationen-Tabellen ---------------------
    # Baut zun√§chst eine Zwischenstruktur auf, um Beziehungen zu generieren
    rel_rows = {}
    for rel, source_table in RELATION_TABLE_SOURCES.items():
        if source_table not in data:
            continue  # Quelltabelle fehlt ‚Üí keine Beziehungen erzeugbar
        rows = data[source_table]
        builder = RELATION_BUILDERS[rel]  # verwendet vordefinierte Builder-Funktion
        rel_rows[rel] = [builder(r) for r in rows]

    # Schreibt die Beziehungsdaten in separate CSV-Dateien
    for rel, rows in rel_rows.items():
        if not rows:
            continue
        with open(out_dir / f"{rel}.csv", "w", newline="", encoding="utf-8") as f_out:
            writer = csv.DictWriter(f_out, fieldnames=rows[0].keys())
            writer.writeheader()
            writer.writerows(rows)

    # Gibt alle erstellten CSV-Dateien als Liste zur√ºck
    return sorted(out_dir.glob("*.csv"))


def wait_for_bolt(uri="bolt://127.0.0.1:7687", auth=("neo4j","superpassword55"),
                  timeout=120, delay=2):
    """
    Wartet auf die erfolgreiche Erreichbarkeit des Neo4j-Bolt-Endpunkts.

    Parameter:
        uri (str): Bolt-Verbindungs-URI (Standard: localhost mit Port 7687)
        auth (tuple): Tuple aus Benutzername und Passwort f√ºr den Login
        timeout (int): Maximale Wartezeit in Sekunden
        delay (int): Wartezeit zwischen den Verbindungsversuchen in Sekunden
    """

    t0 = time.time()  # Startzeit zur Berechnung des Timeout

    # Wiederholt Verbindungsversuche bis zum Ablauf der maximalen Wartezeit
    while time.time() - t0 < timeout:
        try:
            # Baut eine Verbindung zur Neo4j-Instanz √ºber den Bolt-Protokolltreiber auf
            with GraphDatabase.driver(uri, auth=auth) as drv:
                with drv.session() as s:
                    # F√ºhrt eine einfache Testabfrage aus, um die Betriebsbereitschaft zu pr√ºfen
                    s.run("RETURN 1").consume()
            
            # Gibt eine Erfolgsmeldung aus, wenn Neo4j bereit ist
            print("‚úÖ Neo4j ist bereit.")
            return

        except ServiceUnavailable:
            # Wenn Neo4j noch nicht erreichbar ist, wird kurz gewartet und erneut versucht
            time.sleep(delay)

    # Wird nach Ablauf des Timeouts ausgel√∂st, falls Neo4j nicht verf√ºgbar ist
    raise RuntimeError("‚ùå Neo4j kam nicht hoch ‚Äì Timeout!")


def run_neo4j_import():
    # Gibt eine Statusmeldung zum Start des Importvorgangs aus
    print("üì¶ Importiere CSV-Dateien in Neo4j (Docker) ...")

    # Ermittelt die absoluten Pfade f√ºr das lokale CSV-Verzeichnis und das persistente Datenverzeichnis
    host_import_path = str(CSV_DIR.resolve())
    data_volume_path = str((Path(__file__).resolve().parent / "neo4j_data").resolve())

    # Basis-Befehl zum Starten des Neo4j-Admin-Importprozesses im Docker-Container
    cmd = [
        "docker", "run", "--rm", "--user", "7474:7474",  # Ausf√ºhrung unter dem Neo4j-User (UID/GID)
        "-v", f"{host_import_path}:/var/lib/neo4j/import",  # Bindet Importverzeichnis ins Container-Dateisystem ein
        "-v", f"{data_volume_path}:/data",                  # Bindet das Datenverzeichnis zur Speicherung ein
        IMAGE_NAME,                                         # Verwendetes Neo4j-Docker-Image
        "neo4j-admin", "database", "import", "full",        # Vollst√§ndiger Datenbankimport √ºber Admin-Tool
        "--overwrite-destination=true",                     # √úberschreibt bestehende Datenbank (falls vorhanden)
        "--verbose",                                        # Aktiviert detaillierte Konsolenausgabe
        "--normalize-types=false"                           # Deaktiviert automatische Typanpassung
    ]

    # üîÅ F√ºgt ggf. vorhandene statische CSV-Dateien f√ºr definierte Knoten (Nodes) hinzu
    static_nodes = {"Product": "Product.csv", "Category": "Category.csv"}
    static_relationships = ["product_categories"]

    for label, file_name in static_nodes.items():
        node_file = CSV_DIR / file_name
        if node_file.exists():
            cmd.append(f"--nodes={label}=/var/lib/neo4j/import/{file_name}")

    # üîÅ F√ºgt ggf. vorhandene statische CSV-Dateien f√ºr Beziehungen hinzu
    for rel in static_relationships:
        rel_file = CSV_DIR / f"{rel}.csv"
        if rel_file.exists():
            cmd.append(f"--relationships={rel}=/var/lib/neo4j/import/{rel}.csv")

    # üîÅ Dynamisch generierte Knotendateien aus dem vorherigen JSON-Konvertierungsprozess einbinden
    for table, label in NODE_TYPES.items():
        node_file = CSV_DIR / f"{table}.csv"
        if node_file.exists():
            cmd.append(f"--nodes={label}=/var/lib/neo4j/import/{table}.csv")

    # üîÅ Dynamisch generierte Beziehungsdateien hinzuf√ºgen
    for rel in RELATION_BUILDERS:
        rel_file = CSV_DIR / f"{rel}.csv"
        if rel_file.exists():
            cmd.append(f"--relationships={rel}=/var/lib/neo4j/import/{rel}.csv")

    # Beendet den Befehl mit dem Namen der zu erstellenden Datenbank ("neo4j")
    cmd += ["--", "neo4j"]

    # F√ºhrt den vollst√§ndigen Importbefehl aus; bricht bei Fehler ab (check=True)
    subprocess.run(cmd, check=True)

    # Gibt eine Best√§tigung √ºber den erfolgreichen Abschluss aus
    print("‚úÖ Import abgeschlossen.")


def cleanup():
    # Gibt eine Statusmeldung aus, dass die tempor√§ren CSV-Dateien gel√∂scht werden
    print("üßπ L√∂sche CSV-Dateien ...")

    # Durchsucht das Zielverzeichnis nach allen CSV-Dateien und l√∂scht sie einzeln
    for file in CSV_DIR.glob("*.csv"):
        file.unlink()  # entfernt die Datei vom Dateisystem

    # L√∂scht anschlie√üend das gesamte Verzeichnis, in dem die CSV-Dateien lagen
    shutil.rmtree(CSV_DIR)


def reset_database_directory():
    # Bestimmt den Pfad zum lokalen Datenverzeichnis der Neo4j-Instanz
    db_path = Path(__file__).resolve().parent / "neo4j_data"

    # Pr√ºft, ob der Ordner bereits existiert und ein Verzeichnis ist
    if db_path.exists() and db_path.is_dir():
        print("üß® Entferne bestehenden Neo4j-Datenbank-Ordner ...")
        shutil.rmtree(db_path)  # L√∂scht das gesamte Verzeichnis rekursiv
        print("‚úÖ Alter Datenbankordner entfernt.")

    # Erstellt ein neues, leeres Verzeichnis f√ºr die Datenbank
    db_path.mkdir(parents=True, exist_ok=True)


def _folder_size_mb(path: Path) -> float:
    """
    Liefert die Gr√∂√üe eines Ordners in MB.
    1Ô∏è‚É£  Versuch via `du -sb`, weil es bei Docker-Setups praktisch immer vorhanden ist.
    2Ô∏è‚É£  Fallback: rekursiv per os.walk ‚Äì funktioniert auch auf Windows, ist aber langsamer.
    """
    try:
        size_bytes = int(subprocess.check_output(["du", "-sb", str(path)]).split()[0])
    except Exception:                                      # z. B. 'du' nicht verf√ºgbar
        size_bytes = 0
        for root, _, files in os.walk(path):
            size_bytes += sum((Path(root) / f).stat().st_size for f in files)
    return round(size_bytes / (1024 * 1024), 1)            # eine Nachkommastelle

def log_volume_size(variant: str, users: int,
                    volume_path: Path,
                    out_csv: Path = (BASE_DIR / ".." / "results" / "volume_sizes.csv")) -> None:
    """
    H√§ngt eine Zeile  variant,users,volume_mb  an die Ergebnis-CSV an.

    Parameters
    ----------
    variant      : Kurzbezeichnung (z. B. 'pg_normal', 'neo_optimized')
    users        : Zahl der in dieser Runde importierten Users
    volume_path  : Host-Pfad des gemounteten Volumes (Ordner, nicht Container-ID!)
    out_csv      : Zieldatei; wird angelegt, falls sie noch nicht existiert
    """
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    volume_mb = _folder_size_mb(volume_path)

    # Datei neu anlegen ‚Üí Header schreiben; sonst anh√§ngen
    write_header = not out_csv.exists()
    with out_csv.open("a", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        if write_header:
            w.writerow(["variant", "users", "volume_mb"])
        w.writerow([variant, users, volume_mb])
    print(f"üíæ  Volume-Gr√∂√üe protokolliert: {variant} | {users} | {volume_mb} MB")


def main():
    # Initialisiert einen Argumentparser f√ºr Kommandozeilenargumente
    parser = argparse.ArgumentParser()
    parser.add_argument("--file-id", type=int, required=True, 
                        help="Numerischer Suffix der zu importierenden JSON-Datei (z.‚ÄØB. 'users_3.json')")
    parser.add_argument("--json-dir", type=str, default="../output",
                        help="Pfad zum Verzeichnis mit den vorbereiteten JSON-Dateien")
    
    args = parser.parse_args()  # Parst die √ºbergebenen Argumente
    
    # Zusammensetzen des vollst√§ndigen Dateipfads basierend auf der √ºbergebenen file-id
    json_file = Path(args.json_dir) / f"users_{args.file_id}.json"

    # 1. Entfernt ggf. vorhandene Datenbankdaten und legt ein frisches Verzeichnis an
    reset_database_directory()

    # 2. Beendet laufende Neo4j-Container (falls vorhanden), um Konflikte zu vermeiden
    stop_neo4j_container()

    # 3. Konvertiert die JSON-Daten in tabellenbasierte CSV-Dateien f√ºr den Import
    convert_json_to_csv_refactored(json_file, CSV_DIR)

    # 4. F√ºhrt den vollst√§ndigen CSV-Import in die Neo4j-Datenbank durch
    run_neo4j_import()

    # 5. Entfernt tempor√§re CSV-Dateien, um die Arbeitsumgebung aufzur√§umen
    cleanup()

    # 6. Startet die Neo4j-Datenbank im Docker-Container und wartet auf vollst√§ndige Verf√ºgbarkeit
    start_neo4j_container()

    # 7. Disk-Footprint des frischen Volumes festhalten
    data_volume_path = Path(__file__).resolve().parent / "neo4j_data"
    log_volume_size(variant="neo_optimized",
                    users=args.file_id,           
                    volume_path=data_volume_path)


# Stellt sicher, dass die main()-Funktion nur ausgef√ºhrt wird,
# wenn das Skript direkt gestartet wird (nicht bei Modulimport)
if __name__ == "__main__":
    main()